{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qX8GCc_HAYVP",
        "outputId": "2c215627-bdb7-429d-f715-f2ea5c911052"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "okPIG5eRAh5n",
        "outputId": "334c124c-657c-4027-d7ae-fbdab4485b10"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting efficientnet-pytorch\n",
            "  Downloading efficientnet_pytorch-0.7.1.tar.gz (21 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from efficientnet-pytorch) (2.5.1+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->efficientnet-pytorch) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch->efficientnet-pytorch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->efficientnet-pytorch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->efficientnet-pytorch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->efficientnet-pytorch) (2024.10.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch->efficientnet-pytorch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch->efficientnet-pytorch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch->efficientnet-pytorch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch->efficientnet-pytorch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch->efficientnet-pytorch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch->efficientnet-pytorch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch->efficientnet-pytorch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch->efficientnet-pytorch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch->efficientnet-pytorch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->efficientnet-pytorch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->efficientnet-pytorch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch->efficientnet-pytorch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch->efficientnet-pytorch) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->efficientnet-pytorch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->efficientnet-pytorch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->efficientnet-pytorch) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m45.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m60.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: efficientnet-pytorch\n",
            "  Building wheel for efficientnet-pytorch (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for efficientnet-pytorch: filename=efficientnet_pytorch-0.7.1-py3-none-any.whl size=16424 sha256=a4154745e020ce5031237423105db02753ed591d0b80b8ce8d0b42674945d697\n",
            "  Stored in directory: /root/.cache/pip/wheels/8b/6f/9b/231a832f811ab6ebb1b32455b177ffc6b8b1cd8de19de70c09\n",
            "Successfully built efficientnet-pytorch\n",
            "Installing collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, efficientnet-pytorch\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed efficientnet-pytorch-0.7.1 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ],
      "source": [
        "!pip install efficientnet-pytorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "NYyYBeKECqu0",
        "outputId": "53ff7321-473b-4800-ec4e-0ed0c5e7a57d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting gradio\n",
            "  Downloading gradio-5.20.1-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting aiofiles<24.0,>=22.0 (from gradio)\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.7.1)\n",
            "Collecting fastapi<1.0,>=0.115.2 (from gradio)\n",
            "  Downloading fastapi-0.115.11-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting ffmpy (from gradio)\n",
            "  Downloading ffmpy-0.5.0-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting gradio-client==1.7.2 (from gradio)\n",
            "  Downloading gradio_client-1.7.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting groovy~=0.1 (from gradio)\n",
            "  Downloading groovy-0.1.2-py3-none-any.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.1.6)\n",
            "Collecting markupsafe~=2.0 (from gradio)\n",
            "  Downloading MarkupSafe-2.1.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (1.26.4)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.10.15)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio) (24.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (11.1.0)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.10.6)\n",
            "Collecting pydub (from gradio)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting python-multipart>=0.0.18 (from gradio)\n",
            "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (6.0.2)\n",
            "Collecting ruff>=0.9.3 (from gradio)\n",
            "  Downloading ruff-0.9.10-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
            "Collecting safehttpx<0.2.0,>=0.1.6 (from gradio)\n",
            "  Downloading safehttpx-0.1.6-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting semantic-version~=2.0 (from gradio)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting starlette<1.0,>=0.40.0 (from gradio)\n",
            "  Downloading starlette-0.46.1-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting tomlkit<0.14.0,>=0.12.0 (from gradio)\n",
            "  Downloading tomlkit-0.13.2-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.15.2)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.12.2)\n",
            "Collecting uvicorn>=0.14.0 (from gradio)\n",
            "  Downloading uvicorn-0.34.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.7.2->gradio) (2024.10.0)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.7.2->gradio) (14.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (3.17.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (4.67.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0->gradio) (2.27.2)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (2.3.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Downloading gradio-5.20.1-py3-none-any.whl (62.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 MB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio_client-1.7.2-py3-none-any.whl (322 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.1/322.1 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading fastapi-0.115.11-py3-none-any.whl (94 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.9/94.9 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading groovy-0.1.2-py3-none-any.whl (14 kB)\n",
            "Downloading MarkupSafe-2.1.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (28 kB)\n",
            "Downloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
            "Downloading ruff-0.9.10-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m38.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading safehttpx-0.1.6-py3-none-any.whl (8.7 kB)\n",
            "Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Downloading starlette-0.46.1-py3-none-any.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tomlkit-0.13.2-py3-none-any.whl (37 kB)\n",
            "Downloading uvicorn-0.34.0-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ffmpy-0.5.0-py3-none-any.whl (6.0 kB)\n",
            "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Installing collected packages: pydub, uvicorn, tomlkit, semantic-version, ruff, python-multipart, markupsafe, groovy, ffmpy, aiofiles, starlette, safehttpx, gradio-client, fastapi, gradio\n",
            "  Attempting uninstall: markupsafe\n",
            "    Found existing installation: MarkupSafe 3.0.2\n",
            "    Uninstalling MarkupSafe-3.0.2:\n",
            "      Successfully uninstalled MarkupSafe-3.0.2\n",
            "Successfully installed aiofiles-23.2.1 fastapi-0.115.11 ffmpy-0.5.0 gradio-5.20.1 gradio-client-1.7.2 groovy-0.1.2 markupsafe-2.1.5 pydub-0.25.1 python-multipart-0.0.20 ruff-0.9.10 safehttpx-0.1.6 semantic-version-2.10.0 starlette-0.46.1 tomlkit-0.13.2 uvicorn-0.34.0\n"
          ]
        }
      ],
      "source": [
        "pip install --upgrade gradio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QI_gSE5DAiBK"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append('/content/drive/MyDrive/Deepfake')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5EUjQ1QfAt27"
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "import cv2\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.models import efficientnet_b0, resnet18\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import torchaudio\n",
        "from torchaudio.transforms import MelSpectrogram\n",
        "import io\n",
        "import os\n",
        "import librosa\n",
        "import pandas as pd\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.applications.xception import Xception, preprocess_input, decode_predictions\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from torchvision.models import vit_b_16\n",
        "import torch.nn as nn\n",
        "from torchvision import models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jLFwBBXrAt_S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe35c587-f1f8-43a6-d668-b7fe680ea19b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "\u001b[1m87910968/87910968\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        }
      ],
      "source": [
        "from video_model_combined import sequence_prediction, classify_video_tensorflow, detect_deepfake1, detect_deepfake2, detect_deepfake3, detect_deepfake4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tssSpQXZAxIm"
      },
      "outputs": [],
      "source": [
        "from typing import final\n",
        "import statistics\n",
        "import numpy as np\n",
        "import torch\n",
        "import cv2  # Make sure you have OpenCV installed\n",
        "from blazeface import BlazeFace  # Local blazeface.py file\n",
        "from face_extract import FaceExtractor  # Local face_extract.py file\n",
        "from read_video import VideoReader\n",
        "from fornet import EfficientNetAutoAttB4  # Local fornet.py file\n",
        "from fornet import EfficientNetB4  # Local fornet.py file\n",
        "\n",
        "# Model 2 - PyTorch EfficientNetAutoAttB4ST model\n",
        "def classify_video_pytorch_efficientnet_autoatt(video_path, model_path, num_frames=30):\n",
        "    score = detect_deepfake1(video_path, model_path, num_frames)\n",
        "    score_list = score.flatten().tolist()\n",
        "    final_score = statistics.mode(score_list)\n",
        "    print(final_score)\n",
        "    threshold = 0.5  # Adjust this value based on your model's expected output\n",
        "    return \"Deepfake\" if final_score > threshold else \"Authentic\"\n",
        "\n",
        "# Model 3 - PyTorch EfficientNetB4ST model\n",
        "def classify_video_pytorch_efficientnet_b4st(video_path, model_path, num_frames=30):\n",
        "    score = detect_deepfake2(video_path, model_path, num_frames)\n",
        "    score_list = score.flatten().tolist()\n",
        "    final_score = score.mean()\n",
        "    print(final_score)\n",
        "    threshold = 0.5  # Adjust this value based on your model's expected output\n",
        "    return \"Deepfake\" if final_score > threshold else \"Authentic\"\n",
        "\n",
        "# Model 4 - PyTorch EfficientNetAutoAttB4 model\n",
        "def classify_video_pytorch_efficientnet_autoatt_model4(video_path, model_path, num_frames=30):\n",
        "    score = detect_deepfake3(video_path, model_path, num_frames)\n",
        "    score_list = score.flatten().tolist()\n",
        "    final_score = score.mean()\n",
        "    print(final_score)\n",
        "    threshold = 0.5  # Adjust this value based on your model's expected output\n",
        "    return \"Deepfake\" if final_score > threshold else \"Authentic\"\n",
        "\n",
        "# Model 5 - PyTorch EfficientNetB4 model\n",
        "def classify_video_pytorch_efficientnet_b4_model5(video_path, model_path, num_frames=30):\n",
        "    score = detect_deepfake4(video_path, model_path, num_frames)\n",
        "    score_list = score.flatten().tolist()\n",
        "    final_score = score.mean()\n",
        "    print(final_score)\n",
        "    threshold = 0.5  # Adjust this value based on your model's expected output\n",
        "    return \"Deepfake\" if final_score > threshold else \"Authentic\"\n",
        "\n",
        "# Ensemble using majority voting\n",
        "def ensemble_vote(video_path, tensorflow_model_path, pytorch_model1_path, pytorch_model2_path, pytorch_model4_path, pytorch_model5_path):\n",
        "    # Predictions from each model\n",
        "    prediction1 = classify_video_tensorflow(video_path)\n",
        "    prediction2 = classify_video_pytorch_efficientnet_autoatt(video_path, pytorch_model1_path)\n",
        "    prediction3 = classify_video_pytorch_efficientnet_b4st(video_path, pytorch_model2_path)\n",
        "    prediction4 = classify_video_pytorch_efficientnet_autoatt_model4(video_path, pytorch_model4_path)\n",
        "    prediction5 = classify_video_pytorch_efficientnet_b4_model5(video_path, pytorch_model5_path)\n",
        "\n",
        "    # Collect the predictions\n",
        "    predictions = [prediction1, prediction2, prediction3, prediction4, prediction5]\n",
        "\n",
        "    # Apply majority voting\n",
        "    final_prediction = statistics.mode(predictions)  # Returns the most frequent prediction\n",
        "    final_prediction = predictions[0][0]\n",
        "    return final_prediction, predictions  # Return individual predictions and final result\n",
        "\n",
        "\n",
        "tensorflow_model_path = '/content/drive/MyDrive/Deepfake/video_model.h5'\n",
        "pytorch_model1_path = '/content/drive/MyDrive/Deepfake/bestval.pth'\n",
        "pytorch_model2_path = '/content/drive/MyDrive/Deepfake/B4ST.pth'\n",
        "pytorch_model4_path = '/content/drive/MyDrive/Deepfake/autoATTB4.pth'  # Model 4 path\n",
        "pytorch_model5_path = '/content/drive/MyDrive/Deepfake/B4.pth' # Model 5 path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vTnh79r5AxLm"
      },
      "outputs": [],
      "source": [
        "def audio_deepfake_detector(audio_path):\n",
        "    y, sr = librosa.load(audio_path, sr=None)\n",
        "\n",
        "    # Calculate the number of seconds in the audio\n",
        "    total_duration = librosa.get_duration(y=y, sr=sr)\n",
        "    total_seconds = int(np.ceil(total_duration))  # Round up to ensure all seconds are covered\n",
        "\n",
        "    # Initialize an empty list to store feature dictionaries for each second\n",
        "    features_list = []\n",
        "\n",
        "    # Iterate over each second\n",
        "    for start_sec in range(total_seconds):\n",
        "        # Extract one second of audio\n",
        "        y_sec = y[start_sec * sr : (start_sec + 1) * sr]\n",
        "\n",
        "        # Extract features for this second\n",
        "        chromagram = librosa.feature.chroma_stft(y=y_sec, sr=sr).mean()\n",
        "        spectral_centroid = librosa.feature.spectral_centroid(y=y_sec, sr=sr).mean()\n",
        "        spectral_bandwidth = librosa.feature.spectral_bandwidth(y=y_sec, sr=sr).mean()\n",
        "        spectral_rolloff = librosa.feature.spectral_rolloff(y=y_sec, sr=sr).mean()\n",
        "        zero_crossing_rate = librosa.feature.zero_crossing_rate(y=y_sec).mean()\n",
        "        rms = librosa.feature.rms(y=y_sec).mean()\n",
        "        mfccs = librosa.feature.mfcc(y=y_sec, sr=sr, n_mfcc=20)\n",
        "\n",
        "        # Create a dictionary for features of this second\n",
        "        features_dict = {\n",
        "            'chromagram': chromagram,\n",
        "            'spectral_centroid': spectral_centroid,\n",
        "            'spectral_bandwidth': spectral_bandwidth,\n",
        "            'spectral_rolloff': spectral_rolloff,\n",
        "            'zero_crossing_rate': zero_crossing_rate,\n",
        "            'rms': rms,\n",
        "        }\n",
        "\n",
        "        # Add MFCCs to the dictionary with appropriate keys\n",
        "        for i, mfcc in enumerate(mfccs, 1):\n",
        "            features_dict[f'mfcc_{i}'] = mfcc.mean()\n",
        "\n",
        "        # Add the features dictionary to the list\n",
        "        features_list.append(features_dict)\n",
        "\n",
        "    # Convert the list of dictionaries to a Pandas DataFrame\n",
        "    df = pd.DataFrame(features_list)\n",
        "\n",
        "    # Average the features across all seconds\n",
        "    averaged_features = df.mean().to_frame().T\n",
        "\n",
        "    # Convert DataFrame to NumPy array\n",
        "    tester_array = averaged_features.to_numpy()\n",
        "\n",
        "    # Reshape array to be 3D (samples, time steps, features)\n",
        "    tester = tester_array.reshape(1, tester_array.shape[1], 1)\n",
        "\n",
        "    # Load the audio model\n",
        "    audio_model_path = \"/content/drive/MyDrive/Deepfake/Audio_dEEPfake.h5\"\n",
        "    audio_model = tf.keras.models.load_model(audio_model_path)\n",
        "\n",
        "    # Make a prediction\n",
        "    result_model = audio_model.predict(tester)\n",
        "\n",
        "\n",
        "    # Determine if the audio is real or a deepfake\n",
        "    if result_model >= 0.5:\n",
        "        score = (result_model[0][0])\n",
        "        return \"Real\", float(score)\n",
        "    else:\n",
        "        score = 1-(result_model[0][0] )\n",
        "        return \"Deepfake\", float(score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nsiKJKVyAxOW"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xFTWtCgMdlVF",
        "outputId": "b8e88299-6584-4d0b-98f5-8afb5a865386"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model file verified: /content/drive/MyDrive/Deepfake/Meso4_DF.h5\n",
            "Model file verified: /content/drive/MyDrive/Deepfake/Meso4_F2F.h5\n",
            "Model file verified: /content/drive/MyDrive/Deepfake/MesoInception_DF.h5\n",
            "Model file verified: /content/drive/MyDrive/Deepfake/MesoInception_F2F.h5\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "import h5py\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.layers import (Conv2D, BatchNormalization, LeakyReLU, MaxPooling2D,\n",
        "                                     Flatten, Dense, Dropout, Input, Concatenate)\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "# ================================\n",
        "# 1. Define Model Paths\n",
        "# ================================\n",
        "\n",
        "# Paths to the model weight files uploaded to Colab\n",
        "meso4_df_path = '/content/drive/MyDrive/Deepfake/Meso4_DF.h5'\n",
        "meso4_f2f_path = '/content/drive/MyDrive/Deepfake/Meso4_F2F.h5'\n",
        "meso_inception_df_path = '/content/drive/MyDrive/Deepfake/MesoInception_DF.h5'\n",
        "meso_inception_f2f_path = '/content/drive/MyDrive/Deepfake/MesoInception_F2F.h5'\n",
        "\n",
        "# ================================\n",
        "# 2. Verify and Load Models\n",
        "# ================================\n",
        "\n",
        "def verify_file(model_path):\n",
        "    if not os.path.exists(model_path):\n",
        "        print(f\"Model file does not exist: {model_path}\")\n",
        "        return False\n",
        "    try:\n",
        "        with open(model_path, 'rb') as f:\n",
        "            signature = f.read(8)\n",
        "            if signature != b'\\x89HDF\\r\\n\\x1a\\n':\n",
        "                print(f\"Invalid HDF5 signature for file: {model_path}\")\n",
        "                return False\n",
        "        print(f\"Model file verified: {model_path}\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"Error verifying model {model_path}: {e}\")\n",
        "        return False\n",
        "\n",
        "# Verify all model files\n",
        "model_files = [meso4_df_path, meso4_f2f_path, meso_inception_df_path, meso_inception_f2f_path]\n",
        "for path in model_files:\n",
        "    verify_file(path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B_AB2KuweA4W"
      },
      "outputs": [],
      "source": [
        "# ================================\n",
        "# 3. Build Model Architectures\n",
        "# ================================\n",
        "\n",
        "def build_meso4_model(input_shape=(256, 256, 3)):\n",
        "    inputs = Input(shape=input_shape, name='input_2')\n",
        "\n",
        "    # Block 1\n",
        "    x = Conv2D(8, (3, 3), padding='same', name='conv2d_5')(inputs)\n",
        "    x = BatchNormalization(name='batch_normalization_5')(x)\n",
        "    x = LeakyReLU(alpha=0.1, name='leaky_re_lu_3')(x)\n",
        "    x = MaxPooling2D(pool_size=(2, 2), name='max_pooling2d_5')(x)\n",
        "\n",
        "    # Block 2\n",
        "    x = Conv2D(8, (5, 5), padding='same', name='conv2d_6')(x)\n",
        "    x = BatchNormalization(name='batch_normalization_6')(x)\n",
        "    x = LeakyReLU(alpha=0.1, name='leaky_re_lu_4')(x)\n",
        "    x = MaxPooling2D(pool_size=(2, 2), name='max_pooling2d_6')(x)\n",
        "\n",
        "    # Block 3\n",
        "    x = Conv2D(16, (5, 5), padding='same', name='conv2d_7')(x)\n",
        "    x = BatchNormalization(name='batch_normalization_7')(x)\n",
        "    x = LeakyReLU(alpha=0.1, name='leaky_re_lu_5')(x)\n",
        "    x = MaxPooling2D(pool_size=(2, 2), name='max_pooling2d_7')(x)\n",
        "\n",
        "    # Block 4\n",
        "    x = Conv2D(16, (5, 5), padding='same', name='conv2d_8')(x)\n",
        "    x = BatchNormalization(name='batch_normalization_8')(x)\n",
        "    x = LeakyReLU(alpha=0.1, name='leaky_re_lu_6')(x)\n",
        "    x = MaxPooling2D(pool_size=(2, 2), name='max_pooling2d_8')(x)\n",
        "\n",
        "    # Flatten and Dense Layers\n",
        "    x = Flatten(name='flatten_2')(x)\n",
        "    x = Dense(16, activation='relu', name='dense_3')(x)\n",
        "    x = Dropout(0.5, name='dropout_3')(x)\n",
        "    outputs = Dense(1, activation='sigmoid', name='dense_4')(x)\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=outputs, name='meso4_model')\n",
        "    return model\n",
        "\n",
        "def inception_module(x, filters, name=None):\n",
        "    branch1 = Conv2D(filters, (1, 1), padding='same', name=f'{name}_conv2d_1')(x)\n",
        "    branch1 = BatchNormalization(name=f'{name}_batch_normalization_1')(branch1)\n",
        "    branch1 = LeakyReLU(alpha=0.1, name=f'{name}_leaky_re_lu_1')(branch1)\n",
        "\n",
        "    branch2 = Conv2D(filters, (1, 1), padding='same', name=f'{name}_conv2d_2')(x)\n",
        "    branch2 = BatchNormalization(name=f'{name}_batch_normalization_2')(branch2)\n",
        "    branch2 = LeakyReLU(alpha=0.1, name=f'{name}_leaky_re_lu_2')(branch2)\n",
        "    branch2 = Conv2D(filters, (3, 3), padding='same', name=f'{name}_conv2d_3')(branch2)\n",
        "    branch2 = BatchNormalization(name=f'{name}_batch_normalization_3')(branch2)\n",
        "    branch2 = LeakyReLU(alpha=0.1, name=f'{name}_leaky_re_lu_3')(branch2)\n",
        "\n",
        "    branch3 = Conv2D(filters, (1, 1), padding='same', name=f'{name}_conv2d_4')(x)\n",
        "    branch3 = BatchNormalization(name=f'{name}_batch_normalization_4')(branch3)\n",
        "    branch3 = LeakyReLU(alpha=0.1, name=f'{name}_leaky_re_lu_4')(branch3)\n",
        "    branch3 = Conv2D(filters, (5, 5), padding='same', name=f'{name}_conv2d_5')(branch3)\n",
        "    branch3 = BatchNormalization(name=f'{name}_batch_normalization_5')(branch3)\n",
        "    branch3 = LeakyReLU(alpha=0.1, name=f'{name}_leaky_re_lu_5')(branch3)\n",
        "\n",
        "    branch4 = MaxPooling2D((3, 3), strides=(1, 1), padding='same', name=f'{name}_max_pooling2d_1')(x)\n",
        "    branch4 = Conv2D(filters, (1, 1), padding='same', name=f'{name}_conv2d_6')(branch4)\n",
        "    branch4 = BatchNormalization(name=f'{name}_batch_normalization_6')(branch4)\n",
        "    branch4 = LeakyReLU(alpha=0.1, name=f'{name}_leaky_re_lu_6')(branch4)\n",
        "\n",
        "    output = Concatenate(axis=-1, name=f'{name}_concatenate_1')([branch1, branch2, branch3, branch4])\n",
        "    return output\n",
        "\n",
        "def build_meso_inception_model(input_shape=(256, 256, 3)):\n",
        "    inputs = Input(shape=input_shape, name='input_1')\n",
        "\n",
        "    x = inception_module(inputs, filters=32, name='inception_1')\n",
        "    x = MaxPooling2D(pool_size=(2, 2), name='max_pooling2d_1')(x)\n",
        "    # Add additional inception modules and layers as needed\n",
        "    # For brevity, we're adding only one inception module here\n",
        "\n",
        "    x = Flatten(name='flatten_1')(x)\n",
        "    x = Dense(16, activation='relu', name='dense_1')(x)\n",
        "    x = Dropout(0.5, name='dropout_1')(x)\n",
        "    outputs = Dense(1, activation='sigmoid', name='dense_2')(x)\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=outputs, name='meso_inception_model')\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ceL6b-SvUBt8",
        "outputId": "df3d8886-4706-4b84-8e7b-60fb18a15ef1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.20.1+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (2.1.5)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch torchvision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sPBG_J7QeymY",
        "outputId": "11ca1e15-6ff1-4d10-8b19-d1e040e57a75"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/activations/leaky_relu.py:41: UserWarning: Argument `alpha` is deprecated. Use `negative_slope` instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/keras/src/legacy/saving/legacy_h5_format.py:513: UserWarning: Skipping loading weights for layer #15 (named dense_3)due to mismatch in shape for weight dense_3/kernel. Weight expects shape (4096, 16). Received saved weight with shape (1024, 16)\n",
            "  _set_weights(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Successfully loaded weights for Meso4_DF\n",
            "✅ Successfully loaded weights for Meso4_F2F\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/legacy/saving/legacy_h5_format.py:513: UserWarning: Skipping loading weights for layer #29 (named dense_1)due to mismatch in shape for weight dense_1/kernel. Weight expects shape (2097152, 16). Received saved weight with shape (1024, 16)\n",
            "  _set_weights(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Successfully loaded weights for MesoInception_DF\n",
            "✅ Successfully loaded weights for MesoInception_F2F\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n",
            "<ipython-input-13-d1a060aab5d5>:32: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  resnet18_model.load_state_dict(torch.load('/content/drive/MyDrive/Deepfake/Image_fine_tuned_resnet18.pth', map_location=device))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Successfully loaded ResNet18 model\n"
          ]
        }
      ],
      "source": [
        "# ================================\n",
        "# 4. Load Models and Weights\n",
        "# ================================\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import models\n",
        "\n",
        "def load_models():\n",
        "    models_dict = {}\n",
        "\n",
        "    # Load Meso4 model\n",
        "    meso4_model = build_meso4_model()\n",
        "    meso4_model.load_weights(meso4_df_path, by_name=True, skip_mismatch=True)\n",
        "    print(\"✅ Successfully loaded weights for Meso4_DF\")\n",
        "    meso4_model.load_weights(meso4_f2f_path, by_name=True, skip_mismatch=True)\n",
        "    print(\"✅ Successfully loaded weights for Meso4_F2F\")\n",
        "    models_dict['meso4'] = meso4_model\n",
        "\n",
        "    # Load MesoInception model\n",
        "    meso_inception_model = build_meso_inception_model()\n",
        "    meso_inception_model.load_weights(meso_inception_df_path, by_name=True, skip_mismatch=True)\n",
        "    print(\"✅ Successfully loaded weights for MesoInception_DF\")\n",
        "    meso_inception_model.load_weights(meso_inception_f2f_path, by_name=True, skip_mismatch=True)\n",
        "    print(\"✅ Successfully loaded weights for MesoInception_F2F\")\n",
        "    models_dict['meso_inception'] = meso_inception_model\n",
        "\n",
        "    # Load ResNet18 model (PyTorch)\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    resnet18_model = models.resnet18(pretrained=False)\n",
        "    num_features = resnet18_model.fc.in_features\n",
        "    resnet18_model.fc = nn.Linear(num_features, 2)\n",
        "    resnet18_model.load_state_dict(torch.load('/content/drive/MyDrive/Deepfake/Image_fine_tuned_resnet18.pth', map_location=device))\n",
        "    resnet18_model = resnet18_model.to(device)\n",
        "    resnet18_model.eval()\n",
        "    models_dict['resnet18'] = resnet18_model\n",
        "    models_dict['resnet18_device'] = device\n",
        "    print(\"✅ Successfully loaded ResNet18 model\")\n",
        "\n",
        "    return models_dict\n",
        "\n",
        "models_dict = load_models()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n1Woo12brpxx"
      },
      "outputs": [],
      "source": [
        "# ================================\n",
        "# 6. Preprocessing Functions\n",
        "# ================================\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "from torch import unsqueeze\n",
        "\n",
        "def preprocess_image_meso(img_path, target_size=(256, 256)):\n",
        "    img = Image.open(img_path).convert('RGB')\n",
        "    img = img.resize(target_size)\n",
        "    img_array = np.array(img).astype('float32') / 255.0\n",
        "    img_array = np.expand_dims(img_array, axis=0)  # Add batch dimension\n",
        "    return img_array\n",
        "\n",
        "def preprocess_image_resnet18(img_path):\n",
        "    image_transforms = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                             std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "    img = Image.open(img_path).convert('RGB')\n",
        "    img_tensor = image_transforms(img).unsqueeze(0)  # Add batch dimension\n",
        "    return img_tensor\n",
        "\n",
        "def display_image(img_path):\n",
        "    img = Image.open(img_path)\n",
        "    plt.imshow(img)\n",
        "    plt.axis('off')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sp9hUqqcrIFh"
      },
      "outputs": [],
      "source": [
        "# ================================\n",
        "# 7. Ensemble Prediction Function\n",
        "# ================================\n",
        "\n",
        "def ensemble_prediction(models_dict, img_path):\n",
        "    scores = {}\n",
        "\n",
        "    # Preprocess images\n",
        "    img_meso = preprocess_image_meso(img_path)\n",
        "    img_resnet18 = preprocess_image_resnet18(img_path)\n",
        "\n",
        "    # Meso4 prediction\n",
        "    meso4_pred = models_dict['meso4'].predict(img_meso)\n",
        "    meso4_score = meso4_pred[0][0]\n",
        "    scores['meso4'] = meso4_score\n",
        "\n",
        "    # MesoInception prediction\n",
        "    meso_inception_pred = models_dict['meso_inception'].predict(img_meso)\n",
        "    meso_inception_score = meso_inception_pred[0][0]\n",
        "    scores['meso_inception'] = meso_inception_score\n",
        "\n",
        "    # ResNet18 prediction\n",
        "    device = models_dict['resnet18_device']\n",
        "    img_resnet18 = img_resnet18.to(device)\n",
        "    with torch.no_grad():\n",
        "        output = models_dict['resnet18'](img_resnet18)\n",
        "        probabilities = torch.nn.functional.softmax(output, dim=1)\n",
        "        resnet18_score = probabilities[0][1].item()  # Probability of being Deepfake\n",
        "    scores['resnet18'] = resnet18_score\n",
        "\n",
        "    # -------------------------------\n",
        "    # Simple Average Ensemble\n",
        "    # -------------------------------\n",
        "    average_score = np.mean([meso4_score, meso_inception_score, resnet18_score])\n",
        "    scores['average_score'] = average_score\n",
        "\n",
        "    if average_score >= 0.5:\n",
        "        average_final_prediction = 'Deepfake'\n",
        "    else:\n",
        "        average_final_prediction = 'Real'\n",
        "    scores['average_final_prediction'] = average_final_prediction\n",
        "\n",
        "    # -------------------------------\n",
        "    # Advanced Ensemble Methods\n",
        "    # -------------------------------\n",
        "\n",
        "    # Weighted Average Ensemble\n",
        "    weights = {\n",
        "        'meso4': 0.3,\n",
        "        'meso_inception': 0.1,\n",
        "        'resnet18': 0.6\n",
        "    }\n",
        "    weighted_average_score = (\n",
        "        meso4_score * weights['meso4'] +\n",
        "        meso_inception_score * weights['meso_inception'] +\n",
        "        resnet18_score * weights['resnet18']\n",
        "    )\n",
        "    scores['weighted_average_score'] = weighted_average_score\n",
        "\n",
        "    if weighted_average_score >= 0.5:\n",
        "        weighted_final_prediction = 'Deepfake'\n",
        "    else:\n",
        "        weighted_final_prediction = 'Real'\n",
        "    scores['weighted_final_prediction'] = weighted_final_prediction\n",
        "\n",
        "    # Majority Voting Ensemble\n",
        "    meso4_prediction = 1 if meso4_score >= 0.5 else 0\n",
        "    meso_inception_prediction = 1 if meso_inception_score >= 0.5 else 0\n",
        "    resnet18_prediction = 1 if resnet18_score >= 0.5 else 0\n",
        "\n",
        "    votes = meso4_prediction + meso_inception_prediction + resnet18_prediction\n",
        "    if votes >= 1:\n",
        "        majority_vote_prediction = 'real'\n",
        "    else:\n",
        "        majority_vote_prediction = 'Deepfake'\n",
        "    scores['majority_vote_prediction'] = majority_vote_prediction\n",
        "\n",
        "    return scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 650
        },
        "id": "f-vQjb09A9Lb",
        "outputId": "cefbda90-7439-49fc-9414-69757302fbc4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://74b0bcfc07bb42876b.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://74b0bcfc07bb42876b.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "def classify(input_file_path, file_type):\n",
        "    try:\n",
        "        file_extension = input_file_path.split('.')[-1].lower()\n",
        "\n",
        "        # Classify videos\n",
        "        if file_type == 'Video':\n",
        "            if file_extension in ['mp4', 'avi', 'mov']:\n",
        "                # Make sure these model paths are defined or passed to the function\n",
        "                final_prediction, individual_predictions = ensemble_vote(\n",
        "                    input_file_path,\n",
        "                    tensorflow_model_path,\n",
        "                    pytorch_model1_path,\n",
        "                    pytorch_model2_path,\n",
        "                    pytorch_model4_path,\n",
        "                    pytorch_model5_path\n",
        "                )\n",
        "                confidence = individual_predictions[0][1]  # Adjust based on your confidence calculation\n",
        "                return final_prediction, confidence\n",
        "\n",
        "        # Classify images using gradio_ensemble_prediction\n",
        "        elif file_type == 'Image':\n",
        "            if file_extension in ['png', 'jpg', 'jpeg']:\n",
        "                # Use the gradio_ensemble_prediction instead of classify_image_resnet18\n",
        "                prediction = gradio_ensemble_prediction(input_file_path)\n",
        "                confidence = \"N/A\"  # Adjust if you need to return confidence as well\n",
        "                return prediction, confidence\n",
        "\n",
        "        # Classify audio\n",
        "        elif file_type == 'Audio':\n",
        "            if file_extension in ['wav', 'mp3']:\n",
        "                prediction, confidence = audio_deepfake_detector(input_file_path)\n",
        "                return prediction, confidence\n",
        "\n",
        "        return \"Invalid file type\", None\n",
        "\n",
        "    except Exception as e:\n",
        "        return \"Error: \" + str(e), None\n",
        "\n",
        "# Define this function outside the classify function\n",
        "def is_image_file(file_path):\n",
        "    valid_extensions = ('.jpg', '.jpeg', '.png', '.gif', '.bmp')\n",
        "    return file_path.lower().endswith(valid_extensions)\n",
        "\n",
        "# Define this function outside the classify function\n",
        "def gradio_ensemble_prediction(image):\n",
        "    if not is_image_file(image):\n",
        "        return \"Please upload an image file (jpg, jpeg, png, gif, bmp)\"\n",
        "\n",
        "    # Ensure models_dict is defined or passed into the function\n",
        "    scores = ensemble_prediction(models_dict, image)\n",
        "\n",
        "    result = (\n",
        "        f\"Meso4 Score: {scores['meso4']}\\n\"\n",
        "        f\"MesoInception Score: {scores['meso_inception']}\\n\"\n",
        "        f\"ResNet18 Score: {scores['resnet18']}\\n\"\n",
        "        f\"Average Score: {scores['average_score']}\\n\"\n",
        "        f\"Final Prediction (Simple Average): {scores['average_final_prediction']}\\n\"\n",
        "        f\"Weighted Average Score: {scores['weighted_average_score']}\\n\"\n",
        "        f\"Final Prediction (Weighted Average): {scores['weighted_final_prediction']}\\n\"\n",
        "        f\"Final Prediction (Majority Voting): {scores['majority_vote_prediction']}\"\n",
        "        f\"Final Prediction (Majority Voting): {scores['majority_vote_prediction']}\"\n",
        "    )\n",
        "    return result\n",
        "\n",
        "# Gradio interface\n",
        "import gradio as gr\n",
        "\n",
        "iface = gr.Interface(\n",
        "    fn=classify,\n",
        "    inputs=[\n",
        "        gr.File(type=\"filepath\", label=\"Upload Image/Video/Audio\"),\n",
        "        gr.Dropdown(choices=['Video', 'Image', 'Audio'], label=\"File Type\")\n",
        "    ],\n",
        "    outputs=[\n",
        "        gr.Textbox(label=\"Prediction\"),\n",
        "        gr.Textbox(label=\"Confidence\")\n",
        "    ],\n",
        "    title=\"Deepfake Detection\",\n",
        "    description=\"Classify images, videos, or audio as Deepfake or Real.\"\n",
        ")\n",
        "\n",
        "# Launch the Gradio interface\n",
        "if __name__ == \"__main__\":\n",
        "    iface.launch()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tV01bQp4A-CO"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d7TZqI_FA-Eu"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4rNkOTR2A-HS"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}